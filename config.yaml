# LLM Configuration
llm_providers:
  openai:
    default_model: "gpt-4o-mini"
    models:
      # GPT-4 models (widely available, cost-effective)
      - "gpt-4o-mini"
      - "gpt-4o"
      - "gpt-4-turbo"
      # GPT-5 models (if available in your API tier)
      - "gpt-5"
      - "gpt-5.1"
      - "gpt-5.2"
      # Note: GPT-5 models may require specific API access tiers
      # Comment out models you don't have access to
    temperature: 0.7  # Realistic deployment temperature
    max_tokens: 4000  # Increased for chain-of-thought responses

  gemini:
    default_model: "gemini-pro"
    models:
      - "gemini-pro"
      - "gemini-1.5-pro"
      - "gemini-1.5-flash"
      - "gemini-1.0-pro"
    temperature: 0.7
    max_tokens: 4000

  anthropic:
    default_model: "claude-sonnet-4-20250514"
    models:
      - "claude-sonnet-4-20250514"
      - "claude-opus-4-5-20251101"
      - "claude-3-5-sonnet-20241022"
      - "claude-3-5-haiku-20241022"
      - "claude-3-opus-20240229"
      - "claude-3-sonnet-20240229"
      - "claude-3-haiku-20240307"
    temperature: 0.7
    max_tokens: 4000

# Pipeline Configuration
pipelines:
  one_shot:
    description: "Single prompt analysis"
    enabled: true
  
  chain_of_thought:
    description: "Step-by-step reasoning approach"
    enabled: true
  
  multi_layer:
    description: "Multi-stage analysis with LLM synthesis"
    enabled: true

  decomposed_algorithmic:
    description: "Criteria-based with algorithmic aggregation (shares criteria evaluation with multi_layer)"
    enabled: true

# Analysis Configuration
analysis:
  output_format: "json"
  save_intermediate_results: true
  results_dir: "results"

